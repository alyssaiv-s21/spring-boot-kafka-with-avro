# spring-boot-kafka-with-avro
Kafka producer and consumer services with Avro serialization

Этот проект демонстрирует работу продюсера и консьюмера Apache Kafka с использованием Spring Boot и сериализации Avro.    
Проект использует кластер из трех брокеров Kafka для высокой доступности и Schema Registry для управления схемами данных.


## Структура проекта
Проект состоит из двух Spring Boot приложений:
 - kafka-producer: Отправляет сообщения в Kafka.
 - kafka-consumer: Читает сообщения из Kafka.

В корне проекта находится файл docker-compose.yml, который описывает инфраструктуру:
 - zookeeper: Координатор для Kafka.
 - broker1, broker2, broker3: Три экземпляра Kafka брокеров.
 - schema-registry: Хранилище для Avro схем.

 ## Запуск проекта

1. **Запуск инфраструктуры**
```bash
docker-compose up -d
```
Эта команда поднимет все необходимые контейнеры в фоновом режиме:
- 3 брокера Kafka
- ZooKeeper
- Schema Registry

2. **Проверка статуса контейнеров**
```bash
docker ps
```

3. **Сборка и запуск приложений с использованием**
Запускаем приложения producer и консьюмер.
Producer будет ждать пользоваельского ввода, после ввода упаковывать полученные данные в Message, и посылать в кафку, Косьюмер читает эти сообщения.

4. ## Конфигурация
Конфигурационные файлы (application.yml) находятся в папках src/main/resources каждого приложения. 
Avro-схемы, используемые для сериализации и десериализации, определены в файлах .avsc. 
Например, в demo-producer/src/main/avro/ находится схема для сообщения. 
При сборке проекта Gradle автоматически генерирует соответствующие Java-классы из этих схем.





 
